{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4a129f53",
   "metadata": {},
   "source": [
    "## Задача про поиск похожих слов по эмбеддингам\n",
    "\n",
    "На основе word2vec/fasttext слоя Embedding реализовать метод поиска ближайших твитов (на вход метода должен приходить запрос (какой-то твит, вопрос) и количество вариантов вывода к примеру 5-ть, ваш метод должен возвращать 5-ть ближайших твитов к этому запросу)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db27f8fe",
   "metadata": {},
   "source": [
    "### Подготовка корпуса данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2f30fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!wget https://www.dropbox.com/s/fnpq3z4bcnoktiv/positive.csv?dl=0\n",
    "#!wget https://www.dropbox.com/s/r6u59ljhhjdg6j0/negative.csv?dl=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d571d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Загружаем данные из CSV-файлов\n",
    "positive_df = pd.read_csv('positive.csv', sep=';')\n",
    "negative_df = pd.read_csv('negative.csv', sep=';')\n",
    "\n",
    "# Объединяем их в одну выборку\n",
    "combined_df = pd.concat([positive_df, negative_df], axis=0)\n",
    "\n",
    "# Сохраняем объединенный датасет в новый CSV-файл, если необходимо\n",
    "combined_df.to_csv('combined_dataset.csv', index=False)\n",
    "\n",
    "#combined_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "301f9fed",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ну любишь или нет? — Я не знаю кто ты бля:D ht...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label\n",
       "0  Да, все-таки он немного похож на него. Но мой ...   1.0\n",
       "1  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...   1.0\n",
       "2  RT @digger2912: \"Кто то в углу сидит и погибае...   1.0\n",
       "3  @irina_dyshkant Вот что значит страшилка :D\\nН...   1.0\n",
       "4  ну любишь или нет? — Я не знаю кто ты бля:D ht...   1.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создание полей для исходного датафрейма\n",
    "Field_df = pd.DataFrame()\n",
    "field_template = \"Field_{}\"\n",
    "\n",
    "# Задаем количество полей, которые нужно создать\n",
    "num_fields = combined_df.shape[1]\n",
    "\n",
    "# Создаем столбцы на основе шаблона\n",
    "for i in range(1, num_fields + 1):\n",
    "    field_name = field_template.format(i)\n",
    "    Field_df[field_name] = None  # Здесь можно указать начальные значения для столбцов\n",
    "\n",
    "Field_df = list(Field_df)\n",
    "\n",
    "combined_df.columns = Field_df\n",
    "combined_df = combined_df.astype('str')\n",
    "\n",
    "# Берем только твиты и лейблы\n",
    "combined_df = combined_df[['Field_4', 'Field_5']]\n",
    "\n",
    "new_column_names = ['tweet', 'label']\n",
    "combined_df.columns = new_column_names\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7f542a",
   "metadata": {},
   "source": [
    "### Предобработка текста"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "45c8ef5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Да, все-таки он немного похож на него. Но мой ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @KatiaCheh: Ну ты идиотка) я испугалась за ...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @digger2912: \"Кто то в углу сидит и погибае...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>@irina_dyshkant Вот что значит страшилка :D\\nН...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ну любишь или нет? — Я не знаю кто ты бля:D ht...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               tweet label\n",
       "0  Да, все-таки он немного похож на него. Но мой ...   1.0\n",
       "1  RT @KatiaCheh: Ну ты идиотка) я испугалась за ...   1.0\n",
       "2  RT @digger2912: \"Кто то в углу сидит и погибае...   1.0\n",
       "3  @irina_dyshkant Вот что значит страшилка :D\\nН...   1.0\n",
       "4  ну любишь или нет? — Я не знаю кто ты бля:D ht...   1.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = combined_df\n",
    "selected_columns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "20fcfa03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\vddav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\vddav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "написать код обработка текстовый данные python   python  nlp\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from pymorphy2 import MorphAnalyzer\n",
    "import re\n",
    "\n",
    "# Инициализируйте библиотеки nltk и pymorphy2\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "morph = MorphAnalyzer()\n",
    "stop_words = set(stopwords.words(\"russian\"))\n",
    "\n",
    "# Пример твита\n",
    "tweet = \"Написал код для обработки текстовых данных в Python! #python #NLP\"\n",
    "\n",
    "# Функция для предобработки текста\n",
    "def preprocess_tweet(tweet):\n",
    "    if isinstance(tweet, str):\n",
    "        # Токенизация: разделение текста на слова или токены\n",
    "        words = nltk.word_tokenize(tweet)\n",
    "        \n",
    "        # Приведение к нижнему регистру\n",
    "        words = [word.lower() for word in words]\n",
    "        \n",
    "        # Удаление знаков препинания и цифр\n",
    "        words = [re.sub(r'[^а-яА-Яa-zA-Z]', '', word) for word in words]\n",
    "        \n",
    "        # Удаление стоп-слов\n",
    "        words = [word for word in words if word not in stop_words]\n",
    "        \n",
    "        # Лемматизация: приведение слов к их базовой форме\n",
    "        words = [morph.parse(word)[0].normal_form for word in words]\n",
    "        \n",
    "        return ' '.join(words)\n",
    "\n",
    "    else:\n",
    "        return words\n",
    "\n",
    "# Применение предобработки к твиту\n",
    "preprocessed_tweet = preprocess_tweet(tweet)\n",
    "print(preprocessed_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "252d992b",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = selected_columns.dropna(subset=['tweet'])\n",
    "selected_columns['tweet'] = selected_columns['tweet'].astype(str)\n",
    "selected_columns['tweet'] = selected_columns['tweet'].apply(preprocess_tweet)\n",
    "#selected_columns.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7f195a8",
   "metadata": {},
   "source": [
    "### Обучение модели Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "def58cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import gensim.downloader as api\n",
    "from gensim.models import FastText\n",
    "\n",
    "# Преобразовываем текст в список списков слов (токенизация уже выполнена)\n",
    "sentences = selected_columns['tweet'].apply(lambda x: x.split()).tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189a3b2c",
   "metadata": {},
   "source": [
    "#### a. Word2Vec (с использованием библиотеки Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a8661e66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec CBOW\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "model.save(\"my_word2vec_model_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3531725d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Skip-gram\n",
    "model = Word2Vec(sentences, vector_size=100, window=3, min_count=1, sg=1, epochs=3)\n",
    "model.save(\"my_word2vec_model_sg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67eed7f",
   "metadata": {},
   "source": [
    "#### b. FastText (с использованием библиотеки Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5fc0edd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText CBOW\n",
    "model = FastText(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "model.save(\"my_fasttext_model_cbow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c91be1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText Skip-gram\n",
    "model = FastText(sentences, vector_size=100, window=3, min_count=1, sg=1, epochs=3)\n",
    "model.save(\"my_fasttext_model_sg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e8d4be09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обученная модель Word2Vec\n",
    "word2vec_model = Word2Vec.load(\"my_word2vec_model_cbow\")\n",
    "\n",
    "# Обученная модель FastText\n",
    "fasttext_model = FastText.load(\"my_fasttext_model_sg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3cc0b6",
   "metadata": {},
   "source": [
    "## Поиск ближайших твитов по запросу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99336a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar_tweets(query, dataset, num_results=5, model_type=\"word2vec\"):\n",
    "    # Проверяем, является ли query строкой, и преобразуйте в строку, если это необходимо\n",
    "    if not isinstance(query, str):\n",
    "        query = str(query)\n",
    "    \n",
    "    # Инициализация вектора запроса\n",
    "    if model_type == \"word2vec\":\n",
    "        model = word2vec_model\n",
    "    elif model_type == \"fasttext\":\n",
    "        model = fasttext_model\n",
    "    else:\n",
    "        raise ValueError(\"Неподдерживаемый тип модели. Допустимые значения: 'word2vec' и 'fasttext'.\")\n",
    "    \n",
    "    query_vector_sum = np.zeros(model.vector_size)\n",
    "    num_words = 0\n",
    "    \n",
    "    # Вычисление суммы векторов для слов в запросе\n",
    "    for word in query.split():\n",
    "        if word in model.wv:\n",
    "            query_vector_sum += model.wv[word]\n",
    "            num_words += 1\n",
    "    \n",
    "    # Если есть слова из запроса в модели, вычислите средний вектор\n",
    "    if num_words > 0:\n",
    "        query_vector = query_vector_sum / num_words\n",
    "    else:\n",
    "        # Если все слова из запроса отсутствуют в модели, верните None\n",
    "        return None\n",
    "    \n",
    "    # Рассчет косинусного расстояния между запросом и всеми твитами\n",
    "    similarities = []\n",
    "    for tweet in dataset['tweet']:\n",
    "        tweet_vector_sum = np.zeros(model.vector_size)\n",
    "        num_tweet_words = 0\n",
    "        for word in tweet.split():\n",
    "            if word in model.wv:\n",
    "                tweet_vector_sum += model.wv[word]\n",
    "                num_tweet_words += 1\n",
    "        if num_tweet_words > 0:\n",
    "            tweet_vector = tweet_vector_sum / num_tweet_words\n",
    "            cosine_similarity = np.dot(query_vector, tweet_vector) / (np.linalg.norm(query_vector) * np.linalg.norm(tweet_vector))\n",
    "            similarities.append((tweet, cosine_similarity))\n",
    "    \n",
    "    # Сортировка твитов по близости к запросу\n",
    "    sorted_similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Верните топ N ближайших твитов\n",
    "    return sorted_similarities[:num_results]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a88f112",
   "metadata": {},
   "source": [
    "### Проверка Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18382d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: \"на улице осень\"\n",
      "Ближайшие твиты:\n",
      "1. Твит: @A_Rainik Привет! Похоже не осень виновата,а Зимородки Созрели.)))\n",
      "   Сходство: 1.0000\n",
      "\n",
      "2. Твит: Солнечная осень в Ставрополе) http://t.co/a1UsljRR3u\n",
      "   Сходство: 1.0000\n",
      "\n",
      "3. Твит: @HZF0HB думала весной)но на март-апрель намечено завершение реформирования МСУ)не рискнут,ибо админресурс будет другим занят)ставлю на осень\n",
      "   Сходство: 0.9967\n",
      "\n",
      "4. Твит: RT @jufyzatugade: осень! мне осень всегда почему-то навевает эту песню   :) она такая грустная и добрая) но нравиццо!\n",
      "   Сходство: 0.9940\n",
      "\n",
      "5. Твит: RT @KulpanovichK: Пришла идея распечатать несколько фотографии с телефона и из инстаграма:) Намного приятнее их расматривать вживую)\n",
      "   Сходство: 0.9836\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"на улице осень\"\n",
    "num_results = 5\n",
    "similar_tweets = find_similar_tweets(query, combined_df, num_results, model_type='word2vec')\n",
    "\n",
    "print(f\"Запрос: \\\"{query}\\\"\")\n",
    "print(\"Ближайшие твиты:\")\n",
    "\n",
    "for i, (tweet, similarity) in enumerate(similar_tweets, 1):\n",
    "    print(f\"{i}. Твит: {tweet}\")\n",
    "    print(f\"   Сходство: {similarity:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfef9e04",
   "metadata": {},
   "source": [
    "### Проверка Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9f4013a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Запрос: \"на улице осень\"\n",
      "Ближайшие твиты:\n",
      "1. Твит: Футбол, зимой, в армии, на плацу ахуено))\n",
      "Я этого дожлался!)\n",
      "   Сходство: 0.9988\n",
      "\n",
      "2. Твит: радуемся снегу на нелюбимом 8 этаже)) #Киев #кпі http://t.co/4ULcbBN9QI\n",
      "   Сходство: 0.9988\n",
      "\n",
      "3. Твит: 2:46 на улице лютая метель :) http://t.co/jIYMOpZ70x\n",
      "   Сходство: 0.9987\n",
      "\n",
      "4. Твит: Эхх, на улице великолепно 8) http://t.co/t7nQGv9ech\n",
      "   Сходство: 0.9987\n",
      "\n",
      "5. Твит: #ЧитаюВзаимно погодка стоит прекрасная)))))))пошли на лыжи народ?))))))\n",
      "   Сходство: 0.9986\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"на улице осень\"\n",
    "num_results = 5\n",
    "similar_tweets = find_similar_tweets(query, combined_df, num_results, model_type='fasttext')\n",
    "\n",
    "print(f\"Запрос: \\\"{query}\\\"\")\n",
    "print(\"Ближайшие твиты:\")\n",
    "\n",
    "for i, (tweet, similarity) in enumerate(similar_tweets, 1):\n",
    "    print(f\"{i}. Твит: {tweet}\")\n",
    "    print(f\"   Сходство: {similarity:.4f}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfde4af",
   "metadata": {},
   "source": [
    "## Вывод\n",
    "\n",
    "На основе моих результатов анализа можно сделать следующие выводы и предположения:\n",
    "\n",
    "1. **Word2Vec лучше подходит для задачи поиска похожих твитов**. Этот вывод можно сделать на того, что Word2Vec предоставляет более точные результаты для вашей конкретной задачи. Это может быть связано с различиями в архитектуре и обучении между Word2Vec и FastText.\n",
    "\n",
    "2. **Различия в архитектуре**. Word2Vec и FastText имеют разные архитектуры. Word2Vec учитывает только слова как целостные сущности, в то время как FastText учитывает подслова (n-граммы). Это может сказываться на способности модели к выявлению семантической схожести между твитами.\n",
    "\n",
    "3. **Комбинирование методов**. Возможно, для улучшения качества поиска похожих твитов можно попробовать комбинировать оба метода. Например, использовать Word2Vec для начального поиска, а затем уточнять результаты с помощью FastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70cdb724",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
